{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started:\n",
    "## A simple driving model training and evaluation pipeline using the Drive360 dataset and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from Drive360 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dataset.py** file contains the 3 classes necessary for creating a Drive360Loader. Using the **config.json** file to specify the location of the csv and data directory, we can generate phase (train, validation, test) specific data loaders that can output samples from each set. Adjust the **dataset.py** to your preferred training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train # of data: 76100\n",
      "Phase: validation # of data: 5007\n",
      "Phase: test # of data: 13618\n",
      "Loaded train loader with the following data available as a dict.\n",
      "Index(['cameraRight', 'cameraFront', 'cameraRear', 'cameraLeft', 'canSteering',\n",
      "       'canSpeed', 'chapter'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataset import Drive360Loader\n",
    "\n",
    "# load the config.json file that specifies data \n",
    "# location parameters and other hyperparameters \n",
    "# required.\n",
    "config = json.load(open('./config.json'))\n",
    "\n",
    "# create a train, validation and test data loader\n",
    "train_loader = Drive360Loader(config, 'train')\n",
    "validation_loader = Drive360Loader(config, 'validation')\n",
    "test_loader = Drive360Loader(config, 'test')\n",
    "\n",
    "# print the data (keys) available for use. See full \n",
    "# description of each data type in the documents.\n",
    "print('Loaded train loader with the following data available as a dict.')\n",
    "print(train_loader.drive360.dataframe.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a basic driving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your driving model. This is specific to your learning framework. \n",
    "\n",
    "Below we give a very basic dummy model that uses the front facing camera and a resnet34 + LSTM architecture to predict canSteering and canSpeed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SomeDrivingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SomeDrivingModel, self).__init__()\n",
    "        final_concat_size = 0\n",
    "        \n",
    "        # Main CNN\n",
    "        cnn = models.resnet34(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(cnn.children())[:-1])\n",
    "        self.intermediate = nn.Sequential(nn.Linear(\n",
    "                          cnn.fc.in_features, 128),\n",
    "                          nn.ReLU())\n",
    "        final_concat_size += 128\n",
    "\n",
    "        # Main LSTM\n",
    "        self.lstm = nn.LSTM(input_size=128,\n",
    "                            hidden_size=64,\n",
    "                            num_layers=3,\n",
    "                            batch_first=False)\n",
    "        final_concat_size += 64\n",
    "        \n",
    "        # Angle Regressor\n",
    "        self.control_angle = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        # Speed Regressor\n",
    "        self.control_speed = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        module_outputs = []\n",
    "        lstm_i = []\n",
    "        # Loop through temporal sequence of\n",
    "        # front facing camera images and pass \n",
    "        # through the cnn.\n",
    "        for k, v in data['cameraFront'].items():\n",
    "            v = v.cuda()\n",
    "            x = self.features(v)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.intermediate(x)\n",
    "            lstm_i.append(x)\n",
    "            # feed the current front facing camera\n",
    "            # output directly into the \n",
    "            # regression networks.\n",
    "            if k == 0:\n",
    "                module_outputs.append(x)\n",
    "\n",
    "        # Feed temporal outputs of CNN into LSTM\n",
    "        i_lstm, _ = self.lstm(torch.stack(lstm_i))\n",
    "        module_outputs.append(i_lstm[-1])\n",
    "        \n",
    "        # Concatenate current image CNN output \n",
    "        # and LSTM output.\n",
    "        x_cat = torch.cat(module_outputs, dim=-1)\n",
    "        \n",
    "        # Feed concatenated outputs into the \n",
    "        # regession networks.\n",
    "        prediction = {'canSteering': torch.squeeze(self.control_angle(x_cat)),\n",
    "                      'canSpeed': torch.squeeze(self.control_speed(x_cat))}\n",
    "        return prediction\n",
    "\n",
    "# Create your own driving model, this is\n",
    "#  a very basic one. \n",
    "model = SomeDrivingModel().cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic training procedure that iterates over the train_loader and feeds each sample into our dummy model, subsequently calculates loss. We kill after 20 batches just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch:      2] loss: 0.65118\n",
      "[epoch: 1, batch:      4] loss: 0.46805\n",
      "[epoch: 1, batch:      6] loss: 0.55829\n",
      "[epoch: 1, batch:      8] loss: 0.50975\n",
      "[epoch: 1, batch:     10] loss: 0.54617\n",
      "[epoch: 1, batch:     12] loss: 0.49688\n",
      "[epoch: 1, batch:     14] loss: 0.51667\n",
      "[epoch: 1, batch:     16] loss: 0.48274\n",
      "[epoch: 1, batch:     18] loss: 0.49431\n",
      "[epoch: 1, batch:     20] loss: 0.54391\n",
      "[epoch: 1, batch:     22] loss: 0.58162\n",
      "[epoch: 1, batch:     24] loss: 0.38899\n",
      "[epoch: 1, batch:     26] loss: 0.51456\n",
      "[epoch: 1, batch:     28] loss: 0.52285\n",
      "[epoch: 1, batch:     30] loss: 0.61976\n",
      "[epoch: 1, batch:     32] loss: 0.64781\n",
      "[epoch: 1, batch:     34] loss: 0.45452\n",
      "[epoch: 1, batch:     36] loss: 0.51503\n",
      "[epoch: 1, batch:     38] loss: 0.39669\n",
      "[epoch: 1, batch:     40] loss: 0.51423\n",
      "[epoch: 1, batch:     42] loss: 0.52808\n",
      "[epoch: 1, batch:     44] loss: 0.52180\n",
      "[epoch: 1, batch:     46] loss: 0.48802\n",
      "[epoch: 1, batch:     48] loss: 0.56890\n",
      "[epoch: 1, batch:     50] loss: 0.52951\n",
      "[epoch: 1, batch:     52] loss: 0.64166\n",
      "[epoch: 1, batch:     54] loss: 0.43853\n",
      "[epoch: 1, batch:     56] loss: 0.49147\n",
      "[epoch: 1, batch:     58] loss: 0.55201\n",
      "[epoch: 1, batch:     60] loss: 0.45625\n",
      "[epoch: 1, batch:     62] loss: 0.46330\n",
      "[epoch: 1, batch:     64] loss: 0.56176\n",
      "[epoch: 1, batch:     66] loss: 0.55278\n",
      "[epoch: 1, batch:     68] loss: 0.47731\n",
      "[epoch: 1, batch:     70] loss: 0.44438\n",
      "[epoch: 1, batch:     72] loss: 0.59679\n",
      "[epoch: 1, batch:     74] loss: 0.51178\n",
      "[epoch: 1, batch:     76] loss: 0.44578\n",
      "[epoch: 1, batch:     78] loss: 0.40280\n",
      "[epoch: 1, batch:     80] loss: 0.41419\n",
      "[epoch: 1, batch:     82] loss: 0.49168\n",
      "[epoch: 1, batch:     84] loss: 0.51414\n",
      "[epoch: 1, batch:     86] loss: 0.57933\n",
      "[epoch: 1, batch:     88] loss: 0.53909\n",
      "[epoch: 1, batch:     90] loss: 0.43031\n",
      "[epoch: 1, batch:     92] loss: 0.48574\n",
      "[epoch: 1, batch:     94] loss: 0.50370\n",
      "[epoch: 1, batch:     96] loss: 0.43927\n",
      "[epoch: 1, batch:     98] loss: 0.48724\n",
      "[epoch: 1, batch:    100] loss: 0.46141\n",
      "[epoch: 1, batch:    102] loss: 0.50897\n",
      "[epoch: 1, batch:    104] loss: 0.51793\n",
      "[epoch: 1, batch:    106] loss: 0.43768\n",
      "[epoch: 1, batch:    108] loss: 0.51597\n",
      "[epoch: 1, batch:    110] loss: 0.50408\n",
      "[epoch: 1, batch:    112] loss: 0.44761\n",
      "[epoch: 1, batch:    114] loss: 0.47101\n",
      "[epoch: 1, batch:    116] loss: 0.58525\n",
      "[epoch: 1, batch:    118] loss: 0.51183\n",
      "[epoch: 1, batch:    120] loss: 0.52144\n",
      "[epoch: 1, batch:    122] loss: 0.49886\n",
      "[epoch: 1, batch:    124] loss: 0.49811\n",
      "[epoch: 1, batch:    126] loss: 0.56550\n",
      "[epoch: 1, batch:    128] loss: 0.63267\n",
      "[epoch: 1, batch:    130] loss: 0.42878\n",
      "[epoch: 1, batch:    132] loss: 0.60259\n",
      "[epoch: 1, batch:    134] loss: 0.54673\n",
      "[epoch: 1, batch:    136] loss: 0.45301\n",
      "[epoch: 1, batch:    138] loss: 0.43441\n",
      "[epoch: 1, batch:    140] loss: 0.48919\n",
      "[epoch: 1, batch:    142] loss: 0.48916\n",
      "[epoch: 1, batch:    144] loss: 0.44589\n",
      "[epoch: 1, batch:    146] loss: 0.43400\n",
      "[epoch: 1, batch:    148] loss: 0.41203\n",
      "[epoch: 1, batch:    150] loss: 0.48966\n",
      "[epoch: 1, batch:    152] loss: 0.61550\n",
      "[epoch: 1, batch:    154] loss: 0.48872\n",
      "[epoch: 1, batch:    156] loss: 0.43200\n",
      "[epoch: 1, batch:    158] loss: 0.53985\n",
      "[epoch: 1, batch:    160] loss: 0.48911\n",
      "[epoch: 1, batch:    162] loss: 0.41879\n",
      "[epoch: 1, batch:    164] loss: 0.53247\n",
      "[epoch: 1, batch:    166] loss: 0.52123\n",
      "[epoch: 1, batch:    168] loss: 0.45063\n",
      "[epoch: 1, batch:    170] loss: 0.55130\n",
      "[epoch: 1, batch:    172] loss: 0.54388\n",
      "[epoch: 1, batch:    174] loss: 0.65624\n",
      "[epoch: 1, batch:    176] loss: 0.55903\n",
      "[epoch: 1, batch:    178] loss: 0.46399\n",
      "[epoch: 1, batch:    180] loss: 0.50858\n",
      "[epoch: 1, batch:    182] loss: 0.57546\n",
      "[epoch: 1, batch:    184] loss: 0.50427\n",
      "[epoch: 1, batch:    186] loss: 0.46761\n",
      "[epoch: 1, batch:    188] loss: 0.44697\n",
      "[epoch: 1, batch:    190] loss: 0.49665\n",
      "[epoch: 1, batch:    192] loss: 0.35629\n",
      "[epoch: 1, batch:    194] loss: 0.54507\n",
      "[epoch: 1, batch:    196] loss: 0.46021\n",
      "[epoch: 1, batch:    198] loss: 0.46810\n",
      "[epoch: 1, batch:    200] loss: 0.55692\n",
      "[epoch: 1, batch:    202] loss: 0.50506\n",
      "[epoch: 1, batch:    204] loss: 0.47647\n",
      "[epoch: 1, batch:    206] loss: 0.49873\n",
      "[epoch: 1, batch:    208] loss: 0.48200\n",
      "[epoch: 1, batch:    210] loss: 0.51241\n",
      "[epoch: 1, batch:    212] loss: 0.57797\n",
      "[epoch: 1, batch:    214] loss: 0.50377\n",
      "[epoch: 1, batch:    216] loss: 0.46950\n",
      "[epoch: 1, batch:    218] loss: 0.53625\n",
      "[epoch: 1, batch:    220] loss: 0.47357\n",
      "[epoch: 1, batch:    222] loss: 0.43953\n",
      "[epoch: 1, batch:    224] loss: 0.36915\n",
      "[epoch: 1, batch:    226] loss: 0.57063\n",
      "[epoch: 1, batch:    228] loss: 0.63203\n",
      "[epoch: 1, batch:    230] loss: 0.60941\n",
      "[epoch: 1, batch:    232] loss: 0.45673\n",
      "[epoch: 1, batch:    234] loss: 0.44664\n",
      "[epoch: 1, batch:    236] loss: 0.40217\n",
      "[epoch: 1, batch:    238] loss: 0.50616\n",
      "[epoch: 1, batch:    240] loss: 0.50614\n",
      "[epoch: 1, batch:    242] loss: 0.53197\n",
      "[epoch: 1, batch:    244] loss: 0.53262\n",
      "[epoch: 1, batch:    246] loss: 0.54697\n",
      "[epoch: 1, batch:    248] loss: 0.55981\n",
      "[epoch: 1, batch:    250] loss: 0.51445\n",
      "[epoch: 1, batch:    252] loss: 0.58525\n",
      "[epoch: 1, batch:    254] loss: 0.51828\n",
      "[epoch: 1, batch:    256] loss: 0.57370\n",
      "[epoch: 1, batch:    258] loss: 0.43551\n",
      "[epoch: 1, batch:    260] loss: 0.45635\n",
      "[epoch: 1, batch:    262] loss: 0.49901\n",
      "[epoch: 1, batch:    264] loss: 0.51433\n",
      "[epoch: 1, batch:    266] loss: 0.43247\n",
      "[epoch: 1, batch:    268] loss: 0.42558\n",
      "[epoch: 1, batch:    270] loss: 0.42144\n",
      "[epoch: 1, batch:    272] loss: 0.52465\n",
      "[epoch: 1, batch:    274] loss: 0.50391\n",
      "[epoch: 1, batch:    276] loss: 0.55914\n",
      "[epoch: 1, batch:    278] loss: 0.45790\n",
      "[epoch: 1, batch:    280] loss: 0.45254\n",
      "[epoch: 1, batch:    282] loss: 0.43276\n",
      "[epoch: 1, batch:    284] loss: 0.56787\n",
      "[epoch: 1, batch:    286] loss: 0.71407\n",
      "[epoch: 1, batch:    288] loss: 0.49472\n",
      "[epoch: 1, batch:    290] loss: 0.48947\n",
      "[epoch: 1, batch:    292] loss: 0.46395\n",
      "[epoch: 1, batch:    294] loss: 0.42993\n",
      "[epoch: 1, batch:    296] loss: 0.47810\n",
      "[epoch: 1, batch:    298] loss: 0.43464\n",
      "[epoch: 1, batch:    300] loss: 0.44939\n",
      "[epoch: 1, batch:    302] loss: 0.45869\n",
      "[epoch: 1, batch:    304] loss: 0.49742\n",
      "[epoch: 1, batch:    306] loss: 0.53355\n",
      "[epoch: 1, batch:    308] loss: 0.42228\n",
      "[epoch: 1, batch:    310] loss: 0.48880\n",
      "[epoch: 1, batch:    312] loss: 0.53059\n",
      "[epoch: 1, batch:    314] loss: 0.57199\n",
      "[epoch: 1, batch:    316] loss: 0.46751\n",
      "[epoch: 1, batch:    318] loss: 0.50544\n",
      "[epoch: 1, batch:    320] loss: 0.45158\n",
      "[epoch: 1, batch:    322] loss: 0.56253\n",
      "[epoch: 1, batch:    324] loss: 0.52702\n",
      "[epoch: 1, batch:    326] loss: 0.57245\n",
      "[epoch: 1, batch:    328] loss: 0.46880\n",
      "[epoch: 1, batch:    330] loss: 0.38901\n",
      "[epoch: 1, batch:    332] loss: 0.52163\n",
      "[epoch: 1, batch:    334] loss: 0.52803\n",
      "[epoch: 1, batch:    336] loss: 0.51013\n",
      "[epoch: 1, batch:    338] loss: 0.53682\n",
      "[epoch: 1, batch:    340] loss: 0.52369\n",
      "[epoch: 1, batch:    342] loss: 0.50862\n",
      "[epoch: 1, batch:    344] loss: 0.44464\n",
      "[epoch: 1, batch:    346] loss: 0.42280\n",
      "[epoch: 1, batch:    348] loss: 0.42679\n",
      "[epoch: 1, batch:    350] loss: 0.50785\n",
      "[epoch: 1, batch:    352] loss: 0.58388\n",
      "[epoch: 1, batch:    354] loss: 0.47135\n",
      "[epoch: 1, batch:    356] loss: 0.35450\n",
      "[epoch: 1, batch:    358] loss: 0.39393\n",
      "[epoch: 1, batch:    360] loss: 0.40562\n",
      "[epoch: 1, batch:    362] loss: 0.47921\n",
      "[epoch: 1, batch:    364] loss: 0.47167\n",
      "[epoch: 1, batch:    366] loss: 0.52648\n",
      "[epoch: 1, batch:    368] loss: 0.58977\n",
      "[epoch: 1, batch:    370] loss: 0.51910\n",
      "[epoch: 1, batch:    372] loss: 0.45189\n",
      "[epoch: 1, batch:    374] loss: 0.47745\n",
      "[epoch: 1, batch:    376] loss: 0.50812\n",
      "[epoch: 1, batch:    378] loss: 0.44954\n",
      "[epoch: 1, batch:    380] loss: 0.48113\n",
      "[epoch: 1, batch:    382] loss: 0.53397\n",
      "[epoch: 1, batch:    384] loss: 0.55159\n",
      "[epoch: 1, batch:    386] loss: 0.40804\n",
      "[epoch: 1, batch:    388] loss: 0.55571\n",
      "[epoch: 1, batch:    390] loss: 0.56418\n",
      "[epoch: 1, batch:    392] loss: 0.57404\n",
      "[epoch: 1, batch:    394] loss: 0.54454\n",
      "[epoch: 1, batch:    396] loss: 0.54095\n",
      "[epoch: 1, batch:    398] loss: 0.50263\n",
      "[epoch: 1, batch:    400] loss: 0.42977\n",
      "[epoch: 1, batch:    402] loss: 0.33931\n",
      "[epoch: 1, batch:    404] loss: 0.42707\n",
      "[epoch: 1, batch:    406] loss: 0.50637\n",
      "[epoch: 1, batch:    408] loss: 0.40974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch:    410] loss: 0.61401\n",
      "[epoch: 1, batch:    412] loss: 0.48249\n",
      "[epoch: 1, batch:    414] loss: 0.48640\n",
      "[epoch: 1, batch:    416] loss: 0.49733\n",
      "[epoch: 1, batch:    418] loss: 0.38890\n",
      "[epoch: 1, batch:    420] loss: 0.44931\n",
      "[epoch: 1, batch:    422] loss: 0.46144\n",
      "[epoch: 1, batch:    424] loss: 0.63946\n",
      "[epoch: 1, batch:    426] loss: 0.42807\n",
      "[epoch: 1, batch:    428] loss: 0.56803\n",
      "[epoch: 1, batch:    430] loss: 0.50101\n",
      "[epoch: 1, batch:    432] loss: 0.54863\n",
      "[epoch: 1, batch:    434] loss: 0.51390\n",
      "[epoch: 1, batch:    436] loss: 0.55863\n",
      "[epoch: 1, batch:    438] loss: 0.49379\n",
      "[epoch: 1, batch:    440] loss: 0.50432\n",
      "[epoch: 1, batch:    442] loss: 0.77530\n",
      "[epoch: 1, batch:    444] loss: 0.43683\n",
      "[epoch: 1, batch:    446] loss: 0.54562\n",
      "[epoch: 1, batch:    448] loss: 0.40003\n",
      "[epoch: 1, batch:    450] loss: 0.54643\n",
      "[epoch: 1, batch:    452] loss: 0.38169\n",
      "[epoch: 1, batch:    454] loss: 0.50623\n",
      "[epoch: 1, batch:    456] loss: 0.65832\n",
      "[epoch: 1, batch:    458] loss: 0.54523\n",
      "[epoch: 1, batch:    460] loss: 0.43510\n",
      "[epoch: 1, batch:    462] loss: 0.43568\n",
      "[epoch: 1, batch:    464] loss: 0.53325\n",
      "[epoch: 1, batch:    466] loss: 0.68588\n",
      "[epoch: 1, batch:    468] loss: 0.51316\n",
      "[epoch: 1, batch:    470] loss: 0.44347\n",
      "[epoch: 1, batch:    472] loss: 0.50513\n",
      "[epoch: 1, batch:    474] loss: 0.49066\n",
      "[epoch: 1, batch:    476] loss: 0.42767\n",
      "[epoch: 1, batch:    478] loss: 0.49608\n",
      "[epoch: 1, batch:    480] loss: 0.37283\n",
      "[epoch: 1, batch:    482] loss: 0.48893\n",
      "[epoch: 1, batch:    484] loss: 0.49271\n",
      "[epoch: 1, batch:    486] loss: 0.43461\n",
      "[epoch: 1, batch:    488] loss: 0.42247\n",
      "[epoch: 1, batch:    490] loss: 0.51626\n",
      "[epoch: 1, batch:    492] loss: 0.49301\n",
      "[epoch: 1, batch:    494] loss: 0.49243\n",
      "[epoch: 1, batch:    496] loss: 0.48394\n",
      "[epoch: 1, batch:    498] loss: 0.36987\n",
      "[epoch: 1, batch:    500] loss: 0.43308\n",
      "[epoch: 1, batch:    502] loss: 0.46637\n",
      "[epoch: 1, batch:    504] loss: 0.58310\n",
      "[epoch: 1, batch:    506] loss: 0.42422\n",
      "[epoch: 1, batch:    508] loss: 0.53195\n",
      "[epoch: 1, batch:    510] loss: 0.43877\n",
      "[epoch: 1, batch:    512] loss: 0.53011\n",
      "[epoch: 1, batch:    514] loss: 0.40081\n",
      "[epoch: 1, batch:    516] loss: 0.46605\n",
      "[epoch: 1, batch:    518] loss: 0.45943\n",
      "[epoch: 1, batch:    520] loss: 0.51401\n",
      "[epoch: 1, batch:    522] loss: 0.43647\n",
      "[epoch: 1, batch:    524] loss: 0.49678\n",
      "[epoch: 1, batch:    526] loss: 0.50887\n",
      "[epoch: 1, batch:    528] loss: 0.58417\n",
      "[epoch: 1, batch:    530] loss: 0.43166\n",
      "[epoch: 1, batch:    532] loss: 0.49263\n",
      "[epoch: 1, batch:    534] loss: 0.55693\n",
      "[epoch: 1, batch:    536] loss: 0.40236\n",
      "[epoch: 1, batch:    538] loss: 0.43067\n",
      "[epoch: 1, batch:    540] loss: 0.47858\n",
      "[epoch: 1, batch:    542] loss: 0.54078\n",
      "[epoch: 1, batch:    544] loss: 0.48396\n",
      "[epoch: 1, batch:    546] loss: 0.39711\n",
      "[epoch: 1, batch:    548] loss: 0.58078\n",
      "[epoch: 1, batch:    550] loss: 0.42582\n",
      "[epoch: 1, batch:    552] loss: 0.53665\n",
      "[epoch: 1, batch:    554] loss: 0.44017\n",
      "[epoch: 1, batch:    556] loss: 0.50990\n",
      "[epoch: 1, batch:    558] loss: 0.44637\n",
      "[epoch: 1, batch:    560] loss: 0.41889\n",
      "[epoch: 1, batch:    562] loss: 0.44000\n",
      "[epoch: 1, batch:    564] loss: 0.44944\n",
      "[epoch: 1, batch:    566] loss: 0.36353\n",
      "[epoch: 1, batch:    568] loss: 0.44938\n",
      "[epoch: 1, batch:    570] loss: 0.46983\n",
      "[epoch: 1, batch:    572] loss: 0.56037\n",
      "[epoch: 1, batch:    574] loss: 0.35424\n",
      "[epoch: 1, batch:    576] loss: 0.37585\n",
      "[epoch: 1, batch:    578] loss: 0.40514\n",
      "[epoch: 1, batch:    580] loss: 0.47828\n",
      "[epoch: 1, batch:    582] loss: 0.44644\n",
      "[epoch: 1, batch:    584] loss: 0.50911\n",
      "[epoch: 1, batch:    586] loss: 0.41947\n",
      "[epoch: 1, batch:    588] loss: 0.44921\n",
      "[epoch: 1, batch:    590] loss: 0.36002\n",
      "[epoch: 1, batch:    592] loss: 0.40814\n",
      "[epoch: 1, batch:    594] loss: 0.41091\n",
      "[epoch: 1, batch:    596] loss: 0.42332\n",
      "[epoch: 1, batch:    598] loss: 0.47745\n",
      "[epoch: 1, batch:    600] loss: 0.49094\n",
      "[epoch: 1, batch:    602] loss: 0.48882\n",
      "[epoch: 1, batch:    604] loss: 0.46698\n",
      "[epoch: 1, batch:    606] loss: 0.53034\n",
      "[epoch: 1, batch:    608] loss: 0.43886\n",
      "[epoch: 1, batch:    610] loss: 0.54260\n",
      "[epoch: 1, batch:    612] loss: 0.48343\n",
      "[epoch: 1, batch:    614] loss: 0.48108\n",
      "[epoch: 1, batch:    616] loss: 0.53088\n",
      "[epoch: 1, batch:    618] loss: 0.53679\n",
      "[epoch: 1, batch:    620] loss: 0.44207\n",
      "[epoch: 1, batch:    622] loss: 0.47588\n",
      "[epoch: 1, batch:    624] loss: 0.49358\n",
      "[epoch: 1, batch:    626] loss: 0.50615\n",
      "[epoch: 1, batch:    628] loss: 0.57501\n",
      "[epoch: 1, batch:    630] loss: 0.55845\n",
      "[epoch: 1, batch:    632] loss: 0.42884\n",
      "[epoch: 1, batch:    634] loss: 0.47131\n",
      "[epoch: 1, batch:    636] loss: 0.46955\n",
      "[epoch: 1, batch:    638] loss: 0.48556\n",
      "[epoch: 1, batch:    640] loss: 0.34616\n",
      "[epoch: 1, batch:    642] loss: 0.45921\n",
      "[epoch: 1, batch:    644] loss: 0.54032\n",
      "[epoch: 1, batch:    646] loss: 0.44674\n",
      "[epoch: 1, batch:    648] loss: 0.36785\n",
      "[epoch: 1, batch:    650] loss: 0.65286\n",
      "[epoch: 1, batch:    652] loss: 0.54021\n",
      "[epoch: 1, batch:    654] loss: 0.43389\n",
      "[epoch: 1, batch:    656] loss: 0.52930\n",
      "[epoch: 1, batch:    658] loss: 0.39045\n",
      "[epoch: 1, batch:    660] loss: 0.51936\n",
      "[epoch: 1, batch:    662] loss: 0.40502\n",
      "[epoch: 1, batch:    664] loss: 0.46273\n",
      "[epoch: 1, batch:    666] loss: 0.51906\n",
      "[epoch: 1, batch:    668] loss: 0.48404\n",
      "[epoch: 1, batch:    670] loss: 0.46945\n",
      "[epoch: 1, batch:    672] loss: 0.42394\n",
      "[epoch: 1, batch:    674] loss: 0.47390\n",
      "[epoch: 1, batch:    676] loss: 0.42562\n",
      "[epoch: 1, batch:    678] loss: 0.55057\n",
      "[epoch: 1, batch:    680] loss: 0.45873\n",
      "[epoch: 1, batch:    682] loss: 0.52896\n",
      "[epoch: 1, batch:    684] loss: 0.37839\n",
      "[epoch: 1, batch:    686] loss: 0.50142\n",
      "[epoch: 1, batch:    688] loss: 0.48067\n",
      "[epoch: 1, batch:    690] loss: 0.60466\n",
      "[epoch: 1, batch:    692] loss: 0.51050\n",
      "[epoch: 1, batch:    694] loss: 0.46092\n",
      "[epoch: 1, batch:    696] loss: 0.41827\n",
      "[epoch: 1, batch:    698] loss: 0.52086\n",
      "[epoch: 1, batch:    700] loss: 0.44417\n",
      "[epoch: 1, batch:    702] loss: 0.45887\n",
      "[epoch: 1, batch:    704] loss: 0.41306\n",
      "[epoch: 1, batch:    706] loss: 0.45888\n",
      "[epoch: 1, batch:    708] loss: 0.45304\n",
      "[epoch: 1, batch:    710] loss: 0.52757\n",
      "[epoch: 1, batch:    712] loss: 0.50604\n",
      "[epoch: 1, batch:    714] loss: 0.54858\n",
      "[epoch: 1, batch:    716] loss: 0.52392\n",
      "[epoch: 1, batch:    718] loss: 0.50117\n",
      "[epoch: 1, batch:    720] loss: 0.41878\n",
      "[epoch: 1, batch:    722] loss: 0.51253\n",
      "[epoch: 1, batch:    724] loss: 0.40258\n",
      "[epoch: 1, batch:    726] loss: 0.46322\n",
      "[epoch: 1, batch:    728] loss: 0.40526\n",
      "[epoch: 1, batch:    730] loss: 0.47486\n",
      "[epoch: 1, batch:    732] loss: 0.47719\n",
      "[epoch: 1, batch:    734] loss: 0.54658\n",
      "[epoch: 1, batch:    736] loss: 0.40868\n",
      "[epoch: 1, batch:    738] loss: 0.53415\n",
      "[epoch: 1, batch:    740] loss: 0.46824\n",
      "[epoch: 1, batch:    742] loss: 0.46794\n",
      "[epoch: 1, batch:    744] loss: 0.52523\n",
      "[epoch: 1, batch:    746] loss: 0.49518\n",
      "[epoch: 1, batch:    748] loss: 0.44028\n",
      "[epoch: 1, batch:    750] loss: 0.38483\n",
      "[epoch: 1, batch:    752] loss: 0.47970\n",
      "[epoch: 1, batch:    754] loss: 0.49940\n",
      "[epoch: 1, batch:    756] loss: 0.61322\n",
      "[epoch: 1, batch:    758] loss: 0.46845\n",
      "[epoch: 1, batch:    760] loss: 0.55456\n",
      "[epoch: 1, batch:    762] loss: 0.41895\n",
      "[epoch: 1, batch:    764] loss: 0.46665\n",
      "[epoch: 1, batch:    766] loss: 0.45997\n",
      "[epoch: 1, batch:    768] loss: 0.49346\n",
      "[epoch: 1, batch:    770] loss: 0.60078\n",
      "[epoch: 1, batch:    772] loss: 0.52359\n",
      "[epoch: 1, batch:    774] loss: 0.43991\n",
      "[epoch: 1, batch:    776] loss: 0.47245\n",
      "[epoch: 1, batch:    778] loss: 0.45754\n",
      "[epoch: 1, batch:    780] loss: 0.46886\n",
      "[epoch: 1, batch:    782] loss: 0.49076\n",
      "[epoch: 1, batch:    784] loss: 0.58583\n",
      "[epoch: 1, batch:    786] loss: 0.48803\n",
      "[epoch: 1, batch:    788] loss: 0.43452\n",
      "[epoch: 1, batch:    790] loss: 0.41437\n",
      "[epoch: 1, batch:    792] loss: 0.34547\n",
      "[epoch: 1, batch:    794] loss: 0.48255\n",
      "[epoch: 1, batch:    796] loss: 0.39702\n",
      "[epoch: 1, batch:    798] loss: 0.54262\n",
      "[epoch: 1, batch:    800] loss: 0.52872\n",
      "[epoch: 1, batch:    802] loss: 0.39974\n",
      "[epoch: 1, batch:    804] loss: 0.46409\n",
      "[epoch: 1, batch:    806] loss: 0.50540\n",
      "[epoch: 1, batch:    808] loss: 0.47903\n",
      "[epoch: 1, batch:    810] loss: 0.42128\n",
      "[epoch: 1, batch:    812] loss: 0.48110\n",
      "[epoch: 1, batch:    814] loss: 0.37724\n",
      "[epoch: 1, batch:    816] loss: 0.45144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch:    818] loss: 0.37257\n",
      "[epoch: 1, batch:    820] loss: 0.36149\n",
      "[epoch: 1, batch:    822] loss: 0.45499\n",
      "[epoch: 1, batch:    824] loss: 0.43222\n",
      "[epoch: 1, batch:    826] loss: 0.55024\n",
      "[epoch: 1, batch:    828] loss: 0.47768\n",
      "[epoch: 1, batch:    830] loss: 0.52694\n",
      "[epoch: 1, batch:    832] loss: 0.48389\n",
      "[epoch: 1, batch:    834] loss: 0.35594\n",
      "[epoch: 1, batch:    836] loss: 0.50364\n",
      "[epoch: 1, batch:    838] loss: 0.52213\n",
      "[epoch: 1, batch:    840] loss: 0.44686\n",
      "[epoch: 1, batch:    842] loss: 0.55195\n",
      "[epoch: 1, batch:    844] loss: 0.26980\n",
      "[epoch: 1, batch:    846] loss: 0.54578\n",
      "[epoch: 1, batch:    848] loss: 0.47905\n",
      "[epoch: 1, batch:    850] loss: 0.50457\n",
      "[epoch: 1, batch:    852] loss: 0.51321\n",
      "[epoch: 1, batch:    854] loss: 0.48372\n",
      "[epoch: 1, batch:    856] loss: 0.42220\n",
      "[epoch: 1, batch:    858] loss: 0.53945\n",
      "[epoch: 1, batch:    860] loss: 0.41226\n",
      "[epoch: 1, batch:    862] loss: 0.40814\n",
      "[epoch: 1, batch:    864] loss: 0.52720\n",
      "[epoch: 1, batch:    866] loss: 0.41307\n",
      "[epoch: 1, batch:    868] loss: 0.40680\n",
      "[epoch: 1, batch:    870] loss: 0.42699\n",
      "[epoch: 1, batch:    872] loss: 0.44106\n",
      "[epoch: 1, batch:    874] loss: 0.54445\n",
      "[epoch: 1, batch:    876] loss: 0.50961\n",
      "[epoch: 1, batch:    878] loss: 0.46065\n",
      "[epoch: 1, batch:    880] loss: 0.49598\n",
      "[epoch: 1, batch:    882] loss: 0.45680\n",
      "[epoch: 1, batch:    884] loss: 0.52271\n",
      "[epoch: 1, batch:    886] loss: 0.42041\n",
      "[epoch: 1, batch:    888] loss: 0.48190\n",
      "[epoch: 1, batch:    890] loss: 0.49757\n",
      "[epoch: 1, batch:    892] loss: 0.65035\n",
      "[epoch: 1, batch:    894] loss: 0.40647\n",
      "[epoch: 1, batch:    896] loss: 0.47073\n",
      "[epoch: 1, batch:    898] loss: 0.45025\n",
      "[epoch: 1, batch:    900] loss: 0.49517\n",
      "[epoch: 1, batch:    902] loss: 0.56683\n",
      "[epoch: 1, batch:    904] loss: 0.47459\n",
      "[epoch: 1, batch:    906] loss: 0.37356\n",
      "[epoch: 1, batch:    908] loss: 0.54322\n",
      "[epoch: 1, batch:    910] loss: 0.49485\n",
      "[epoch: 1, batch:    912] loss: 0.37011\n",
      "[epoch: 1, batch:    914] loss: 0.57834\n",
      "[epoch: 1, batch:    916] loss: 0.40680\n",
      "[epoch: 1, batch:    918] loss: 0.46865\n",
      "[epoch: 1, batch:    920] loss: 0.59756\n",
      "[epoch: 1, batch:    922] loss: 0.48400\n",
      "[epoch: 1, batch:    924] loss: 0.50572\n",
      "[epoch: 1, batch:    926] loss: 0.42389\n",
      "[epoch: 1, batch:    928] loss: 0.43953\n",
      "[epoch: 1, batch:    930] loss: 0.44407\n",
      "[epoch: 1, batch:    932] loss: 0.47713\n",
      "[epoch: 1, batch:    934] loss: 0.33312\n",
      "[epoch: 1, batch:    936] loss: 0.43181\n",
      "[epoch: 1, batch:    938] loss: 0.45201\n",
      "[epoch: 1, batch:    940] loss: 0.44450\n",
      "[epoch: 1, batch:    942] loss: 0.55837\n",
      "[epoch: 1, batch:    944] loss: 0.44929\n",
      "[epoch: 1, batch:    946] loss: 0.64564\n",
      "[epoch: 1, batch:    948] loss: 0.40929\n",
      "[epoch: 1, batch:    950] loss: 0.56701\n",
      "[epoch: 1, batch:    952] loss: 0.40376\n",
      "[epoch: 1, batch:    954] loss: 0.39967\n",
      "[epoch: 1, batch:    956] loss: 0.36612\n",
      "[epoch: 1, batch:    958] loss: 0.48204\n",
      "[epoch: 1, batch:    960] loss: 0.39897\n",
      "[epoch: 1, batch:    962] loss: 0.51321\n",
      "[epoch: 1, batch:    964] loss: 0.52694\n",
      "[epoch: 1, batch:    966] loss: 0.53731\n",
      "[epoch: 1, batch:    968] loss: 0.37898\n",
      "[epoch: 1, batch:    970] loss: 0.47771\n",
      "[epoch: 1, batch:    972] loss: 0.51354\n",
      "[epoch: 1, batch:    974] loss: 0.39849\n",
      "[epoch: 1, batch:    976] loss: 0.39725\n",
      "[epoch: 1, batch:    978] loss: 0.45872\n",
      "[epoch: 1, batch:    980] loss: 0.32770\n",
      "[epoch: 1, batch:    982] loss: 0.36606\n",
      "[epoch: 1, batch:    984] loss: 0.50722\n",
      "[epoch: 1, batch:    986] loss: 0.44949\n",
      "[epoch: 1, batch:    988] loss: 0.39209\n",
      "[epoch: 1, batch:    990] loss: 0.46272\n",
      "[epoch: 1, batch:    992] loss: 0.40843\n",
      "[epoch: 1, batch:    994] loss: 0.45124\n",
      "[epoch: 1, batch:    996] loss: 0.50434\n",
      "[epoch: 1, batch:    998] loss: 0.43087\n",
      "[epoch: 1, batch:   1000] loss: 0.41141\n",
      "[epoch: 1, batch:   1002] loss: 0.37605\n",
      "[epoch: 1, batch:   1004] loss: 0.42928\n",
      "[epoch: 1, batch:   1006] loss: 0.42143\n",
      "[epoch: 1, batch:   1008] loss: 0.40709\n",
      "[epoch: 1, batch:   1010] loss: 0.43737\n",
      "[epoch: 1, batch:   1012] loss: 0.30750\n",
      "[epoch: 1, batch:   1014] loss: 0.50855\n",
      "[epoch: 1, batch:   1016] loss: 0.48315\n",
      "[epoch: 1, batch:   1018] loss: 0.40773\n",
      "[epoch: 1, batch:   1020] loss: 0.55541\n",
      "[epoch: 1, batch:   1022] loss: 0.46184\n",
      "[epoch: 1, batch:   1024] loss: 0.53114\n",
      "[epoch: 1, batch:   1026] loss: 0.39672\n",
      "[epoch: 1, batch:   1028] loss: 0.39558\n",
      "[epoch: 1, batch:   1030] loss: 0.39749\n",
      "[epoch: 1, batch:   1032] loss: 0.42156\n",
      "[epoch: 1, batch:   1034] loss: 0.45351\n",
      "[epoch: 1, batch:   1036] loss: 0.47049\n",
      "[epoch: 1, batch:   1038] loss: 0.35293\n",
      "[epoch: 1, batch:   1040] loss: 0.34011\n",
      "[epoch: 1, batch:   1042] loss: 0.35371\n",
      "[epoch: 1, batch:   1044] loss: 0.37531\n",
      "[epoch: 1, batch:   1046] loss: 0.33400\n",
      "[epoch: 1, batch:   1048] loss: 0.30590\n",
      "[epoch: 1, batch:   1050] loss: 0.41725\n",
      "[epoch: 1, batch:   1052] loss: 0.54512\n",
      "[epoch: 1, batch:   1054] loss: 0.36883\n",
      "[epoch: 1, batch:   1056] loss: 0.49617\n",
      "[epoch: 1, batch:   1058] loss: 0.48325\n",
      "[epoch: 1, batch:   1060] loss: 0.37147\n",
      "[epoch: 1, batch:   1062] loss: 0.39627\n",
      "[epoch: 1, batch:   1064] loss: 0.52456\n",
      "[epoch: 1, batch:   1066] loss: 0.42249\n",
      "[epoch: 1, batch:   1068] loss: 0.52340\n",
      "[epoch: 1, batch:   1070] loss: 0.42126\n",
      "[epoch: 1, batch:   1072] loss: 0.44026\n",
      "[epoch: 1, batch:   1074] loss: 0.50033\n",
      "[epoch: 1, batch:   1076] loss: 0.46382\n",
      "[epoch: 1, batch:   1078] loss: 0.46295\n",
      "[epoch: 1, batch:   1080] loss: 0.42333\n",
      "[epoch: 1, batch:   1082] loss: 0.48385\n",
      "[epoch: 1, batch:   1084] loss: 0.48867\n",
      "[epoch: 1, batch:   1086] loss: 0.42459\n",
      "[epoch: 1, batch:   1088] loss: 0.36626\n",
      "[epoch: 1, batch:   1090] loss: 0.40404\n",
      "[epoch: 1, batch:   1092] loss: 0.43739\n",
      "[epoch: 1, batch:   1094] loss: 0.37640\n",
      "[epoch: 1, batch:   1096] loss: 0.44300\n",
      "[epoch: 1, batch:   1098] loss: 0.46932\n",
      "[epoch: 1, batch:   1100] loss: 0.46143\n",
      "[epoch: 1, batch:   1102] loss: 0.38796\n",
      "[epoch: 1, batch:   1104] loss: 0.42792\n",
      "[epoch: 1, batch:   1106] loss: 0.41470\n",
      "[epoch: 1, batch:   1108] loss: 0.52997\n",
      "[epoch: 1, batch:   1110] loss: 0.54030\n",
      "[epoch: 1, batch:   1112] loss: 0.43278\n",
      "[epoch: 1, batch:   1114] loss: 0.40398\n",
      "[epoch: 1, batch:   1116] loss: 0.48183\n",
      "[epoch: 1, batch:   1118] loss: 0.42583\n",
      "[epoch: 1, batch:   1120] loss: 0.43028\n",
      "[epoch: 1, batch:   1122] loss: 0.50503\n",
      "[epoch: 1, batch:   1124] loss: 0.39818\n",
      "[epoch: 1, batch:   1126] loss: 0.44112\n",
      "[epoch: 1, batch:   1128] loss: 0.51852\n",
      "[epoch: 1, batch:   1130] loss: 0.43604\n",
      "[epoch: 1, batch:   1132] loss: 0.48829\n",
      "[epoch: 1, batch:   1134] loss: 0.39874\n",
      "[epoch: 1, batch:   1136] loss: 0.44076\n",
      "[epoch: 1, batch:   1138] loss: 0.34727\n",
      "[epoch: 1, batch:   1140] loss: 0.35411\n",
      "[epoch: 1, batch:   1142] loss: 0.41546\n",
      "[epoch: 1, batch:   1144] loss: 0.44871\n",
      "[epoch: 1, batch:   1146] loss: 0.43064\n",
      "[epoch: 1, batch:   1148] loss: 0.35660\n",
      "[epoch: 1, batch:   1150] loss: 0.42712\n",
      "[epoch: 1, batch:   1152] loss: 0.34051\n",
      "[epoch: 1, batch:   1154] loss: 0.44114\n",
      "[epoch: 1, batch:   1156] loss: 0.39761\n",
      "[epoch: 1, batch:   1158] loss: 0.46175\n",
      "[epoch: 1, batch:   1160] loss: 0.34789\n",
      "[epoch: 1, batch:   1162] loss: 0.52768\n",
      "[epoch: 1, batch:   1164] loss: 0.42567\n",
      "[epoch: 1, batch:   1166] loss: 0.44356\n",
      "[epoch: 1, batch:   1168] loss: 0.44790\n",
      "[epoch: 1, batch:   1170] loss: 0.36641\n",
      "[epoch: 1, batch:   1172] loss: 0.44963\n",
      "[epoch: 1, batch:   1174] loss: 0.35223\n",
      "[epoch: 1, batch:   1176] loss: 0.40915\n",
      "[epoch: 1, batch:   1178] loss: 0.46428\n",
      "[epoch: 1, batch:   1180] loss: 0.43364\n",
      "[epoch: 1, batch:   1182] loss: 0.44356\n",
      "[epoch: 1, batch:   1184] loss: 0.44530\n",
      "[epoch: 1, batch:   1186] loss: 0.37310\n",
      "[epoch: 1, batch:   1188] loss: 0.42804\n",
      "[epoch: 1, batch:   1190] loss: 0.51434\n",
      "Minutes elapsed: 26.4\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "criterion_speed = nn.SmoothL1Loss()\n",
    "criterion_steering = nn.SmoothL1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data)\n",
    "        \n",
    "        steering_loss = criterion_steering(prediction['canSteering'].cuda(), target['canSteering'].cuda())\n",
    "        speed_loss = criterion_speed(prediction['canSpeed'].cuda(), target['canSpeed'].cuda())\n",
    "        loss = steering_loss + speed_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 2 == 1:  \n",
    "            print('[epoch: %d, batch:  %5d] loss: %.5f' %\n",
    "                  (epoch + 1, batch_idx + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "\n",
    "end = time.time() \n",
    "print(\"Minutes elapsed: {}\".format(round((end - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering Error: 19.375337600708008\n",
      "Speed Error: 57.1062126159668\n",
      "Minutes elapsed: 0.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "start = time.time()\n",
    "model.eval()\n",
    "\n",
    "speed_sum = 0\n",
    "steering_sum = 0\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        prediction = model(data)\n",
    "        # Again only evaluating the canSpeed \n",
    "        # predictions, add canSteering when \n",
    "        # jointly training.\n",
    "        speed_dif = prediction['canSpeed'].cpu() - target['canSpeed']\n",
    "        steering_dif = prediction['canSteering'].cpu() - target['canSteering']\n",
    "        speed_sum += (np.square(speed_dif)).mean()\n",
    "        steering_sum += (np.square(steering_dif)).mean()\n",
    "print(\"Steering Error: {}\\nSpeed Error: {}\".format(float(steering_sum), float(speed_sum)))\n",
    "end = time.time() \n",
    "print(\"Minutes elapsed: {}\".format(round((end - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_targets = config['target']['normalize']\n",
    "target_mean = config['target']['mean']\n",
    "target_std = config['target']['std']\n",
    "\n",
    "def add_results(results, output):\n",
    "    steering = np.squeeze(output['canSteering'].cpu().data.numpy())\n",
    "    speed = np.squeeze(output['canSpeed'].cpu().data.numpy())\n",
    "    if normalize_targets:\n",
    "        steering = (steering*target_std['canSteering'])+target_mean['canSteering']\n",
    "        speed = (speed*target_std['canSpeed'])+target_mean['canSpeed']\n",
    "    if np.isscalar(steering):\n",
    "        steering = [steering]\n",
    "    if np.isscalar(speed):\n",
    "        speed = [speed]\n",
    "    results['canSteering'].extend(steering)\n",
    "    results['canSpeed'].extend(speed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pandas to create a submission file which is simply a 2-column csv with a canSteering and canSpeed prediction for each row in the **drive360_test.csv** a total of 305437 rows/predictions not including the header. See the **sample_submission.csv** file as an example.\n",
    "\n",
    "IMPORTANT: for the test phase indices will start 10s (100 samples) into each chapter this is to allow challenge participants to experiment with different temporal settings of data input. If challenge participants have a greater temporal length than 10s for each training sample, then they must write a custom function here. Please check out the **dataset.py** file for additional explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = './submission.csv'\n",
    "results = {'canSteering': [],\n",
    "           'canSpeed': []}\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        prediction = model(data)\n",
    "        add_results(results, prediction)\n",
    "        \n",
    "df = pd.DataFrame.from_dict(results)\n",
    "df.to_csv(file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
